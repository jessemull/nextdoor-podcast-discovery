# Nextdoor Podcast Discovery Platform

Automatically discover, analyze, and curate interesting Nextdoor posts for podcast content.

## Features

- ğŸ” **Automated Scraping** â€” Daily collection of posts from configured neighborhoods
- ğŸ¤– **LLM Analysis** â€” Score posts on humor, absurdity, drama, relatability using Claude Haiku
- ğŸ” **Semantic Search** â€” Find related posts by meaning using OpenAI embeddings
- ğŸ“Š **Curation Dashboard** â€” Private web UI for browsing, filtering, and selecting posts
- ğŸ“ **Episode Tracking** â€” Mark posts as used, prevent duplicates
- ğŸˆ **Pittsburgh Sports Facts** â€” Random facts for Matt on each login!

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions â”‚â”€â”€â”€â”€â–¶â”‚    Supabase     â”‚â—€â”€â”€â”€â”€â”‚     Vercel      â”‚
â”‚  (Daily Scrape) â”‚     â”‚  (PostgreSQL)   â”‚     â”‚   (Next.js)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                               â”‚
        â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude Haiku   â”‚                           â”‚   Web Dashboard â”‚
â”‚  (Scoring)      â”‚                           â”‚   (Private)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.11+
- Node.js 20+
- Docker (for local database)

### Setup

```bash
# Clone the repository
git clone <repo-url>
cd nextdoor

# Install dependencies
make install

# Start local database
make db-up

# Copy environment variables
cp .env.example .env
# Edit .env with your API keys

# Run the scraper (dry run)
make dev-scraper

# Start the web app
make dev-web
```

## Project Structure

```
nextdoor/
â”œâ”€â”€ scraper/          # Python scraper + LLM workers
â”œâ”€â”€ web/              # Next.js frontend
â”œâ”€â”€ database/         # SQL migrations
â”œâ”€â”€ scripts/          # Utility scripts
â”œâ”€â”€ .github/          # CI/CD workflows
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ PROJECT_PLAN.md   # Full architecture documentation
```

## Documentation

See [PROJECT_PLAN.md](./PROJECT_PLAN.md) for complete architecture documentation including:

- Detailed architecture diagrams
- Database schema
- API specifications
- Deployment guide
- Implementation checklist

## Cost

This project is designed to run on free tiers + minimal API costs:

| Service | Cost |
|---------|------|
| Supabase | Free (500MB) |
| Vercel | Free (Hobby) |
| GitHub Actions | Free (2000 min/mo) |
| Claude Haiku | ~$1/mo |
| OpenAI Embeddings | ~$0.50/mo |
| **Total** | **~$1.50/mo** |

## License

MIT
