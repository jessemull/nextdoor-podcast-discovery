# Nextdoor Podcast Discovery Platform

Automatically discover, analyze, and curate interesting Nextdoor posts for podcast content.

## Features

- ğŸ” **Automated Scraping** â€” Twice-daily collection from Recent and Trending feeds
- ğŸ¤– **LLM Analysis** â€” Score posts on humor, absurdity, drama, relatability using Claude Haiku
- ğŸ” **Semantic Search** â€” Find related posts by meaning using OpenAI embeddings
- ğŸ“Š **Curation Dashboard** â€” Private web UI for browsing, filtering, and selecting posts
- ğŸ“ **Episode Tracking** â€” Mark posts as used, prevent duplicates
- ğŸˆ **Pittsburgh Sports Facts** â€” Random facts for Matt on each login!

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local Linux    â”‚â”€â”€â”€â”€â–¶â”‚    Supabase     â”‚â—€â”€â”€â”€â”€â”‚     Vercel      â”‚
â”‚  (Cron Jobs)    â”‚     â”‚  (PostgreSQL)   â”‚     â”‚   (Next.js)     â”‚
â”‚  - Scrape       â”‚     â”‚  + pgvector     â”‚     â”‚   (Web UI)      â”‚
â”‚  - Score        â”‚     â”‚                  â”‚     â”‚                 â”‚
â”‚  - Embed        â”‚     â”‚                  â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude Haiku   â”‚   â”‚  OpenAI         â”‚
â”‚  (Scoring)      â”‚   â”‚  (Embeddings)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.11+
- Node.js 20+
- Docker (for local database)

### Setup

```bash
# Clone the repository
git clone <repo-url>
cd nextdoor

# Create virtual environment
make venv
source .venv/bin/activate

# Install dependencies
make install

# Start local database
make db-up

# Create environment variable files
touch scraper/.env
touch web/.env.local
# Edit both files with your API keys (see Environment variables below)

# Load scraper env vars into your shell (zsh/bash)
set -a
source scraper/.env
set +a

# Run the scraper (dry run)
make dev-scraper

# Start the web app
make dev-web
```

## Environment variables

Required environment variables are defined in each appâ€™s example file:

- **Scraper:** [scraper/.env.example](./scraper/.env.example) â€” Nextdoor credentials, Supabase, session encryption, Anthropic, OpenAI.
- **Web:** [web/.env.example](./web/.env.example) â€” Supabase, NextAuth, Google OAuth, allowed emails.

Copy to `.env` (scraper) or `.env.local` (web) and fill in your values.

## Project Structure

```
nextdoor/
â”œâ”€â”€ scraper/                # Python scraper + LLM workers
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration
â”‚   â”‚   â”œâ”€â”€ embed.py        # Standalone embedding script
â”‚   â”‚   â”œâ”€â”€ embedder.py     # OpenAI embeddings
â”‚   â”‚   â”œâ”€â”€ exceptions.py   # Custom exceptions
â”‚   â”‚   â”œâ”€â”€ llm_scorer.py   # Claude scoring
â”‚   â”‚   â”œâ”€â”€ main.py         # Entry point
â”‚   â”‚   â”œâ”€â”€ novelty.py      # Shared novelty calculation (scorer + worker)
â”‚   â”‚   â”œâ”€â”€ post_extractor.py
â”‚   â”‚   â”œâ”€â”€ post_storage.py
â”‚   â”‚   â”œâ”€â”€ scraper.py      # Playwright scraper
â”‚   â”‚   â”œâ”€â”€ session_manager.py
â”‚   â”‚   â””â”€â”€ worker.py       # Background job worker
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ web/                    # Next.js frontend
â”‚   â”œâ”€â”€ app/                # Next.js App Router pages
â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”œâ”€â”€ lib/                # Utilities and config
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ database/               # SQL migrations
â”‚   â””â”€â”€ migrations/
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ .github/                # CI/CD workflows
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ PROJECT_PLAN.md         # Full architecture documentation
```

## Scraping policy

The scraper uses configurable delays and does not currently fetch or enforce Nextdoorâ€™s **robots.txt**. If you run this against other domains, consider adding a startup check to fetch and respect robots.txt. See [scraper/README.md](./scraper/README.md) for rate limiting and policy details.

## Documentation

See [PROJECT_PLAN.md](./PROJECT_PLAN.md) for complete architecture documentation including:

- Detailed architecture diagrams
- Database schema
- API specifications
- Deployment guide
- Implementation checklist

## Cost

This project is designed to run on free tiers + minimal API costs:

| Service | Cost |
|---------|------|
| Supabase | Free (500MB) |
| Vercel | Free (Hobby) |
| Local Linux | Free (your hardware) |
| Claude Haiku | ~$1/mo |
| OpenAI Embeddings | ~$0.50/mo |
| **Total** | **~$1.50/mo** |

## License

MIT
