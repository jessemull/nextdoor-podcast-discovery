name: Daily Scrape (Trending)

on:
  schedule:
    - cron: '0 18 * * *'  # 6:00 PM UTC daily
  workflow_dispatch:      # Allow manual trigger

jobs:
  scrape-trending:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          cache-dependency-path: scraper/requirements.txt
          cache: 'pip'
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd scraper
          pip install -r requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          key: playwright-${{ runner.os }}-${{ hashFiles('scraper/requirements.txt') }}
          path: ~/.cache/ms-playwright

      - name: Install Playwright browsers
        run: |
          cd scraper
          playwright install chromium

      - name: Run scraper pipeline (trending feed)
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          NEXTDOOR_EMAIL: ${{ secrets.NEXTDOOR_EMAIL }}
          NEXTDOOR_PASSWORD: ${{ secrets.NEXTDOOR_PASSWORD }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SESSION_ENCRYPTION_KEY: ${{ secrets.SESSION_ENCRYPTION_KEY }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        run: |
          cd scraper
          python -m src.main --feed-type trending --score

      - name: Recount topic frequencies
        env:
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        run: |
          cd scraper
          python -m src.recount_topics

      - name: Generate embeddings
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SESSION_ENCRYPTION_KEY: ${{ secrets.SESSION_ENCRYPTION_KEY }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        run: |
          cd scraper
          python -m src.embed

      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'scraper-failure'
            });
            if (issues.data.length > 0) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: `Trending scrape failed.\n\nRun ID: ${context.runId}\nRun URL: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`
              });
            } else {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'ðŸš¨ Scraper pipeline failed (trending)',
                labels: ['scraper-failure'],
                body: `The daily trending scrape failed.\n\nRun ID: ${context.runId}\nRun URL: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`
              });
            }
