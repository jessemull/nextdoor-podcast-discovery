-- Generated by 'make db-bootstrap'. Source: database/migrations/ 001-037 in order. Do not edit by hand.

-- Initial database schema for Nextdoor Podcast Discovery Platform
-- Run this in Supabase SQL Editor

-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Function to auto-update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Neighborhoods table
CREATE TABLE neighborhoods (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    slug VARCHAR(255) NOT NULL UNIQUE,
    is_active BOOLEAN DEFAULT true,
    weight_modifier FLOAT DEFAULT 1.0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TRIGGER update_neighborhoods_updated_at
    BEFORE UPDATE ON neighborhoods
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Sessions table (for storing encrypted Nextdoor cookies)
CREATE TABLE sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    neighborhood_id UUID REFERENCES neighborhoods(id),
    cookies_encrypted TEXT NOT NULL,
    expires_at TIMESTAMPTZ,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_sessions_expires ON sessions(expires_at);

CREATE TRIGGER update_sessions_updated_at
    BEFORE UPDATE ON sessions
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Posts table
CREATE TABLE posts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    neighborhood_id UUID NOT NULL REFERENCES neighborhoods(id),
    post_id_ext VARCHAR(255) NOT NULL,
    user_id_hash VARCHAR(64),
    text TEXT NOT NULL,
    hash VARCHAR(64) NOT NULL,
    url VARCHAR(512),
    image_urls JSONB DEFAULT '[]',
    posted_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(neighborhood_id, hash)
);

CREATE INDEX idx_posts_neighborhood ON posts(neighborhood_id);
CREATE INDEX idx_posts_hash ON posts(hash);
CREATE INDEX idx_posts_created ON posts(created_at DESC);

-- LLM scores table
CREATE TABLE llm_scores (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    post_id UUID NOT NULL UNIQUE REFERENCES posts(id) ON DELETE CASCADE,
    absurdity FLOAT CHECK (absurdity >= 0 AND absurdity <= 10),
    humor FLOAT CHECK (humor >= 0 AND humor <= 10),
    drama FLOAT CHECK (drama >= 0 AND drama <= 10),
    relatability FLOAT CHECK (relatability >= 0 AND relatability <= 10),
    podcast_score FLOAT,
    tags JSONB DEFAULT '[]',
    summary TEXT,
    processed_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_llm_scores_post ON llm_scores(post_id);

-- Post embeddings table
CREATE TABLE post_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    post_id UUID NOT NULL UNIQUE REFERENCES posts(id) ON DELETE CASCADE,
    embedding VECTOR(1536),
    model VARCHAR(50) DEFAULT 'text-embedding-3-small',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Use HNSW index for better performance
CREATE INDEX idx_embeddings_vector ON post_embeddings 
    USING hnsw (embedding vector_cosine_ops);

-- Rankings table
CREATE TABLE rankings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    post_id UUID NOT NULL UNIQUE REFERENCES posts(id) ON DELETE CASCADE,
    final_score FLOAT NOT NULL DEFAULT 0,
    used_on_episode BOOLEAN DEFAULT false,
    episode_date DATE,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_rankings_score ON rankings(final_score DESC);
CREATE INDEX idx_rankings_unused ON rankings(used_on_episode, final_score DESC);

CREATE TRIGGER update_rankings_updated_at
    BEFORE UPDATE ON rankings
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Settings table
CREATE TABLE settings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    key VARCHAR(100) NOT NULL UNIQUE,
    value JSONB NOT NULL,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TRIGGER update_settings_updated_at
    BEFORE UPDATE ON settings
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Insert default ranking weights
INSERT INTO settings (key, value) VALUES 
    ('ranking_weights', '{"absurdity": 1.0, "drama": 1.0, "humor": 1.0, "relatability": 1.0}');
-- Migration: Update LLM scoring schema
-- Run this in Supabase SQL Editor after 001_initial_schema.sql
--
-- Changes:
-- 1. Update llm_scores to use JSONB scores and categories array
-- 2. Add topic_frequencies table for novelty tracking
-- 3. Add episode tracking columns to posts table
-- 4. Drop rankings table (merged into llm_scores and posts)
-- 5. Update default ranking weights

-- ============================================================================
-- Step 1: Update llm_scores table
-- ============================================================================

-- Drop the old llm_scores table and recreate with new schema
-- (We drop because the structure is significantly different)
DROP TABLE IF EXISTS llm_scores CASCADE;

CREATE TABLE llm_scores (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    post_id UUID NOT NULL UNIQUE REFERENCES posts(id) ON DELETE CASCADE,
    
    -- Individual dimension scores (1-10 scale)
    -- Example: {"absurdity": 8, "drama": 6, "news_value": 3, ...}
    scores JSONB NOT NULL,
    
    -- Topic categories for frequency tracking
    -- Example: ['wildlife', 'humor', 'drama']
    categories TEXT[] NOT NULL DEFAULT '{}',
    
    -- Short summary for quick reference
    summary TEXT,
    
    -- Computed final score (weighted + novelty adjusted)
    final_score FLOAT,
    
    -- Metadata
    model_version TEXT DEFAULT 'claude-3-haiku-20240307',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_llm_scores_post ON llm_scores(post_id);
CREATE INDEX idx_llm_scores_final ON llm_scores(final_score DESC);
CREATE INDEX idx_llm_scores_categories ON llm_scores USING GIN(categories);

-- ============================================================================
-- Step 2: Create topic_frequencies table
-- ============================================================================

CREATE TABLE topic_frequencies (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    category TEXT NOT NULL UNIQUE,
    count_30d INT DEFAULT 0,
    last_updated TIMESTAMPTZ DEFAULT NOW()
);

-- Seed with initial categories
INSERT INTO topic_frequencies (category, count_30d) VALUES
    ('crime', 0),
    ('drama', 0),
    ('humor', 0),
    ('local_news', 0),
    ('lost_pet', 0),
    ('noise', 0),
    ('suspicious', 0),
    ('wildlife', 0);

-- RPC function to atomically increment topic frequency
CREATE OR REPLACE FUNCTION increment_topic_frequency(p_category TEXT, p_increment INT)
RETURNS VOID AS $$
BEGIN
    UPDATE topic_frequencies 
    SET count_30d = count_30d + p_increment,
        last_updated = NOW()
    WHERE category = p_category;
    
    -- Insert if category doesn't exist
    IF NOT FOUND THEN
        INSERT INTO topic_frequencies (category, count_30d, last_updated)
        VALUES (p_category, p_increment, NOW())
        ON CONFLICT (category) DO UPDATE 
        SET count_30d = topic_frequencies.count_30d + p_increment,
            last_updated = NOW();
    END IF;
END;
$$ LANGUAGE plpgsql;

-- RPC function to reset and recount topic frequencies (call daily via cron)
-- This is the safe approach - always accurate, no double-counting issues
CREATE OR REPLACE FUNCTION recount_topic_frequencies()
RETURNS VOID AS $$
BEGIN
    -- Reset all counts
    UPDATE topic_frequencies SET count_30d = 0, last_updated = NOW();
    
    -- Recount from llm_scores within last 30 days
    WITH recent_counts AS (
        SELECT unnest(categories) AS cat, COUNT(*) AS cnt
        FROM llm_scores
        WHERE created_at >= NOW() - INTERVAL '30 days'
        GROUP BY cat
    )
    UPDATE topic_frequencies tf
    SET count_30d = rc.cnt::INT,
        last_updated = NOW()
    FROM recent_counts rc
    WHERE tf.category = rc.cat;
END;
$$ LANGUAGE plpgsql;

-- RPC function to get unscored posts (oldest first for chronological processing)
CREATE OR REPLACE FUNCTION get_unscored_posts(p_limit INT DEFAULT 100)
RETURNS TABLE(id UUID, text TEXT) AS $$
BEGIN
    RETURN QUERY
    SELECT p.id, p.text
    FROM posts p
    LEFT JOIN llm_scores ls ON p.id = ls.post_id
    WHERE ls.id IS NULL
    ORDER BY p.created_at ASC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 3: Add episode tracking to posts table
-- ============================================================================

ALTER TABLE posts ADD COLUMN IF NOT EXISTS used_on_episode BOOLEAN DEFAULT false;
ALTER TABLE posts ADD COLUMN IF NOT EXISTS episode_date DATE;

-- Index for finding unused posts
CREATE INDEX IF NOT EXISTS idx_posts_unused 
    ON posts(used_on_episode) 
    WHERE used_on_episode = false;

-- ============================================================================
-- Step 4: Drop rankings table (no longer needed)
-- ============================================================================

DROP TABLE IF EXISTS rankings CASCADE;

-- ============================================================================
-- Step 5: Update default ranking weights
-- ============================================================================

-- Delete old weights and insert new ones
DELETE FROM settings WHERE key = 'ranking_weights';

INSERT INTO settings (key, value) VALUES 
    ('ranking_weights', '{
        "absurdity": 2.0,
        "drama": 1.5,
        "emotional_intensity": 1.2,
        "discussion_spark": 1.0,
        "news_value": 1.0
    }');

-- Also add novelty configuration
INSERT INTO settings (key, value) VALUES 
    ('novelty_config', '{
        "window_days": 30,
        "min_multiplier": 0.2,
        "max_multiplier": 1.5,
        "frequency_thresholds": {
            "rare": 5,
            "common": 30,
            "very_common": 100
        }
    }')
ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value;

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check tables exist:
-- SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';

-- Check llm_scores structure:
-- \d llm_scores

-- Check topic_frequencies:
-- SELECT * FROM topic_frequencies;

-- Check settings:
-- SELECT * FROM settings;
-- Migration: Add semantic search RPC function
-- Run this in Supabase SQL Editor after 002_llm_scoring_schema.sql
--
-- Adds RPC function for vector similarity search using pgvector

-- ============================================================================
-- Semantic Search Function
-- ============================================================================

-- RPC function to search posts by semantic similarity
-- Takes a query embedding vector and returns similar posts
CREATE OR REPLACE FUNCTION search_posts_by_embedding(
    query_embedding VECTOR(1536),
    similarity_threshold FLOAT DEFAULT 0.5,
    result_limit INT DEFAULT 10
)
RETURNS TABLE(
    id UUID,
    text TEXT,
    similarity DOUBLE PRECISION,
    created_at TIMESTAMPTZ,
    neighborhood_id UUID,
    post_id_ext TEXT,
    url TEXT,
    user_id_hash TEXT,
    image_urls JSONB,
    hash TEXT,
    used_on_episode BOOLEAN,
    episode_date DATE
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        p.id,
        p.text,
        (1 - (pe.embedding <=> query_embedding))::DOUBLE PRECISION AS similarity,
        p.created_at,
        p.neighborhood_id,
        p.post_id_ext::TEXT,
        p.url::TEXT,
        p.user_id_hash::TEXT,
        p.image_urls,
        p.hash::TEXT,
        COALESCE(p.used_on_episode, false),
        p.episode_date
    FROM posts p
    INNER JOIN post_embeddings pe ON p.id = pe.post_id
    WHERE 1 - (pe.embedding <=> query_embedding) >= similarity_threshold
    ORDER BY pe.embedding <=> query_embedding
    LIMIT result_limit;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Verification query (run this to check the function works)
-- ============================================================================

-- Test the function (replace with actual embedding vector):
-- SELECT * FROM search_posts_by_embedding(
--     (SELECT embedding FROM post_embeddings LIMIT 1),
--     0.5,
--     10
-- );
-- Migration: Background Jobs Table
-- Run this in Supabase SQL Editor after 003_semantic_search.sql
--
-- Creates a generic background_jobs table for tracking long-running tasks
-- like recomputing final scores, recounting topic frequencies, etc.

-- ============================================================================
-- Step 1: Create background_jobs table
-- ============================================================================

CREATE TABLE background_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    type TEXT NOT NULL,            -- e.g. 'recompute_final_scores'
    status TEXT NOT NULL,          -- 'pending' | 'running' | 'completed' | 'error' | 'cancelled'
    created_at TIMESTAMPTZ DEFAULT NOW(),
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    created_by TEXT,               -- user email/id (from NextAuth)
    params JSONB,                  -- e.g. { "weights": { ... } }
    progress INTEGER,              -- 0â€“100 or records processed
    total INTEGER,                 -- total records to process (optional)
    error_message TEXT,            -- error details when status = 'error'
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes for efficient job queries
CREATE INDEX idx_background_jobs_type_status ON background_jobs(type, status);
CREATE INDEX idx_background_jobs_status ON background_jobs(status);
CREATE INDEX idx_background_jobs_created ON background_jobs(created_at DESC);

-- Trigger to auto-update updated_at
CREATE TRIGGER update_background_jobs_updated_at
    BEFORE UPDATE ON background_jobs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- ============================================================================
-- Step 2: Add search_defaults to settings (if not exists)
-- ============================================================================

INSERT INTO settings (key, value) VALUES 
    ('search_defaults', '{
        "similarity_threshold": 0.2
    }')
ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value;

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check table exists:
-- SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'background_jobs';

-- Check settings:
-- SELECT * FROM settings WHERE key IN ('ranking_weights', 'search_defaults');
-- Migration: Weight Config Versioning
-- Run this in Supabase SQL Editor after 004_background_jobs.sql
--
-- Implements versioned weight configurations and scores to enable:
-- - Atomic switching between weight configs
-- - Rollback without recompute
-- - Partial recompute
-- - Analytics/comparison between versions
-- - Clean failure handling

-- ============================================================================
-- Step 1: Create weight_configs table
-- ============================================================================

CREATE TABLE weight_configs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- Weight configuration (same structure as settings.ranking_weights)
    weights JSONB NOT NULL,
    
    -- Metadata
    name TEXT,                    -- Optional human-readable name
    created_at TIMESTAMPTZ DEFAULT NOW(),
    created_by TEXT,              -- user email/id (from NextAuth)
    is_active BOOLEAN DEFAULT false,
    
    -- Notes/description for this config
    description TEXT
);

CREATE INDEX idx_weight_configs_active ON weight_configs(is_active) WHERE is_active = true;
CREATE INDEX idx_weight_configs_created ON weight_configs(created_at DESC);

-- Only one active config at a time (enforced by application logic)
-- We'll use settings.active_weight_config_id as the source of truth

-- ============================================================================
-- Step 2: Create post_scores table (versioned final scores)
-- ============================================================================

-- This table stores final_score for each post under each weight config
-- Allows multiple score versions per post (one per weight config)
CREATE TABLE post_scores (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    post_id UUID NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
    weight_config_id UUID NOT NULL REFERENCES weight_configs(id) ON DELETE CASCADE,
    
    -- Computed final score for this post under this weight config
    final_score FLOAT NOT NULL,
    
    -- Metadata
    computed_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Composite unique constraint: one score per post per config
    UNIQUE(post_id, weight_config_id)
);

CREATE INDEX idx_post_scores_config ON post_scores(weight_config_id);
CREATE INDEX idx_post_scores_post ON post_scores(post_id);
CREATE INDEX idx_post_scores_score ON post_scores(weight_config_id, final_score DESC);

-- ============================================================================
-- Step 3: Add active_weight_config_id to settings
-- ============================================================================

-- Add pointer to active weight config (will be set by migration DO block)
-- Initial value is null, will be set to UUID string after creating default config

-- ============================================================================
-- Step 4: Migrate existing final_score to post_scores
-- ============================================================================

-- Create a default weight config from current settings
-- 
-- Transaction Safety:
-- The DO $$ block is automatically wrapped in a transaction. If any step fails,
-- the entire migration will roll back:
-- - Default config creation will be rolled back
-- - Settings update will be rolled back
-- - Score migration will be rolled back
-- This ensures data consistency - either all steps succeed or none do.
--
-- Rollback behavior:
-- If this migration fails partway, you can safely re-run it. The ON CONFLICT
-- clauses prevent duplicate inserts, and the migration is idempotent.
DO $$
DECLARE
    default_config_id UUID;
    current_weights JSONB;
BEGIN
    -- Get current weights from settings
    SELECT value INTO current_weights
    FROM settings
    WHERE key = 'ranking_weights';
    
    -- Create default config
    INSERT INTO weight_configs (weights, name, is_active, description)
    VALUES (
        COALESCE(current_weights, '{"absurdity": 2.5, "discussion_spark": 1.0, "drama": 1.5, "emotional_intensity": 1.2, "news_value": 1.0, "podcast_worthy": 2.5, "readability": 1.2}'::JSONB),
        'Default (Migrated)',
        true,
        'Initial weight config migrated from settings table'
    )
    RETURNING id INTO default_config_id;
    
    -- Set as active in settings (store UUID as JSON string)
    -- Insert or update the setting
    INSERT INTO settings (key, value)
    VALUES ('active_weight_config_id', to_jsonb(default_config_id::TEXT))
    ON CONFLICT (key) DO UPDATE SET value = to_jsonb(default_config_id::TEXT);
    
    -- Migrate existing final_score values to post_scores
    INSERT INTO post_scores (post_id, weight_config_id, final_score, computed_at)
    SELECT 
        ls.post_id,
        default_config_id,
        COALESCE(ls.final_score, 0.0),
        ls.created_at
    FROM llm_scores ls
    WHERE ls.final_score IS NOT NULL
    ON CONFLICT (post_id, weight_config_id) DO NOTHING;
    
    RAISE NOTICE 'Created default weight config % and migrated % scores', default_config_id, (SELECT COUNT(*) FROM post_scores WHERE weight_config_id = default_config_id);
END $$;

-- ============================================================================
-- Step 5: Update background_jobs to track weight_config_id
-- ============================================================================

-- Add weight_config_id to background_jobs params (already JSONB, no schema change needed)
-- Jobs will store weight_config_id in params.weights_config_id

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check weight_configs:
-- SELECT * FROM weight_configs;

-- Check active config:
-- SELECT * FROM settings WHERE key = 'active_weight_config_id';

-- Check post_scores:
-- SELECT COUNT(*) FROM post_scores;

-- Check scores for active config:
-- SELECT COUNT(*) FROM post_scores ps
-- JOIN weight_configs wc ON ps.weight_config_id = wc.id
-- WHERE wc.is_active = true;
-- Migration: RPC Function for Posts with Scores
-- Run this in Supabase SQL Editor after 005_weight_config_versioning.sql
--
-- Creates an RPC function to efficiently join post_scores and llm_scores
-- for the posts API endpoint.

-- ============================================================================
-- RPC Function: get_posts_with_scores
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false
)
RETURNS TABLE(
    post_id UUID,
    final_score FLOAT,
    llm_score_id UUID,
    scores JSONB,
    categories TEXT[],
    summary TEXT,
    model_version TEXT,
    llm_created_at TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ps.post_id,
        ps.final_score,
        ls.id AS llm_score_id,
        ls.scores,
        ls.categories,
        ls.summary,
        ls.model_version,
        ls.created_at AS llm_created_at
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
    ORDER BY ps.final_score DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- RPC Function: get_posts_with_scores_count
-- ============================================================================

-- Separate function for getting total count (needed for pagination)
CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false);
    
    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check functions exist:
-- SELECT routine_name FROM information_schema.routines 
-- WHERE routine_schema = 'public' 
-- AND routine_name IN ('get_posts_with_scores', 'get_posts_with_scores_count');

-- Test the function (replace with actual weight_config_id):
-- SELECT * FROM get_posts_with_scores(
--     'your-weight-config-id-here'::UUID,
--     10,  -- limit
--     0,    -- offset
--     NULL, -- min_score
--     NULL, -- category
--     false -- unused_only
-- );
-- Migration: Optimize has_scores Check
-- Run this in Supabase SQL Editor after 006_posts_with_scores_rpc.sql
--
-- Adds a computed column to weight_configs to track whether a config has scores,
-- eliminating the need to query post_scores table on every API call.
--
-- This optimization improves performance of GET /api/admin/weight-configs
-- especially at scale (millions of post_scores).

-- ============================================================================
-- Step 1: Add has_scores column to weight_configs
-- ============================================================================

ALTER TABLE weight_configs
ADD COLUMN has_scores BOOLEAN DEFAULT false;

CREATE INDEX idx_weight_configs_has_scores ON weight_configs(has_scores) WHERE has_scores = true;

-- ============================================================================
-- Step 2: Update existing configs
-- ============================================================================

-- Set has_scores = true for configs that have post_scores
UPDATE weight_configs wc
SET has_scores = true
WHERE EXISTS (
    SELECT 1
    FROM post_scores ps
    WHERE ps.weight_config_id = wc.id
);

-- ============================================================================
-- Step 3: Create trigger to maintain has_scores
-- ============================================================================

-- Function to update has_scores when post_scores change
CREATE OR REPLACE FUNCTION update_weight_config_has_scores()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        -- Mark config as having scores
        UPDATE weight_configs
        SET has_scores = true
        WHERE id = NEW.weight_config_id;
        RETURN NEW;
    ELSIF TG_OP = 'DELETE' THEN
        -- Check if config still has scores
        UPDATE weight_configs
        SET has_scores = EXISTS (
            SELECT 1
            FROM post_scores ps
            WHERE ps.weight_config_id = OLD.weight_config_id
        )
        WHERE id = OLD.weight_config_id;
        RETURN OLD;
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Trigger on post_scores insert
CREATE TRIGGER trigger_update_has_scores_on_insert
    AFTER INSERT ON post_scores
    FOR EACH ROW
    EXECUTE FUNCTION update_weight_config_has_scores();

-- Trigger on post_scores delete
CREATE TRIGGER trigger_update_has_scores_on_delete
    AFTER DELETE ON post_scores
    FOR EACH ROW
    EXECUTE FUNCTION update_weight_config_has_scores();

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check has_scores column exists:
-- SELECT column_name, data_type FROM information_schema.columns 
-- WHERE table_name = 'weight_configs' AND column_name = 'has_scores';

-- Check triggers exist:
-- SELECT trigger_name FROM information_schema.triggers 
-- WHERE event_object_table = 'post_scores' 
-- AND trigger_name LIKE '%has_scores%';

-- Check has_scores values:
-- SELECT id, name, has_scores FROM weight_configs;
-- Migration: Job Cancellation Support
-- Run this in Supabase SQL Editor after 007_has_scores_optimization.sql
--
-- Adds support for cancelling pending and running background jobs.
-- Workers will check for cancellation status and stop processing gracefully.

-- ============================================================================
-- Step 1: Update background_jobs status constraint
-- ============================================================================

-- The status column already supports 'cancelled' as a valid value
-- (it's TEXT, not an enum). No schema change needed, but we'll add a check
-- constraint for clarity and data integrity.

-- Add check constraint if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_constraint 
        WHERE conname = 'check_background_jobs_status'
    ) THEN
        ALTER TABLE background_jobs
        ADD CONSTRAINT check_background_jobs_status
        CHECK (status IN ('pending', 'running', 'completed', 'error', 'cancelled'));
    END IF;
END $$;

-- ============================================================================
-- Step 2: Add cancelled_at timestamp
-- ============================================================================

ALTER TABLE background_jobs
ADD COLUMN cancelled_at TIMESTAMPTZ;

CREATE INDEX idx_background_jobs_cancelled ON background_jobs(cancelled_at) WHERE cancelled_at IS NOT NULL;

-- ============================================================================
-- Step 3: Add cancelled_by field
-- ============================================================================

ALTER TABLE background_jobs
ADD COLUMN cancelled_by TEXT;

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check constraint exists:
-- SELECT constraint_name FROM information_schema.table_constraints 
-- WHERE table_name = 'background_jobs' AND constraint_name = 'check_background_jobs_status';

-- Check columns exist:
-- SELECT column_name FROM information_schema.columns 
-- WHERE table_name = 'background_jobs' 
-- AND column_name IN ('cancelled_at', 'cancelled_by');
-- Migration: Job Retry Support
-- Run this in Supabase SQL Editor after 008_job_cancellation.sql
--
-- Adds support for automatic retry of failed jobs with configurable max retries.
-- Workers will automatically retry transient failures up to max_retries times.

-- ============================================================================
-- Step 1: Add retry_count column
-- ============================================================================

ALTER TABLE background_jobs
ADD COLUMN retry_count INTEGER DEFAULT 0;

CREATE INDEX idx_background_jobs_retry ON background_jobs(retry_count) WHERE retry_count > 0;

-- ============================================================================
-- Step 2: Add max_retries column (optional, can be set per job type)
-- ============================================================================

ALTER TABLE background_jobs
ADD COLUMN max_retries INTEGER DEFAULT 3;

-- ============================================================================
-- Step 3: Add last_retry_at timestamp
-- ============================================================================

ALTER TABLE background_jobs
ADD COLUMN last_retry_at TIMESTAMPTZ;

-- ============================================================================
-- Verification queries (run these to check the migration worked)
-- ============================================================================

-- Check columns exist:
-- SELECT column_name FROM information_schema.columns 
-- WHERE table_name = 'background_jobs' 
-- AND column_name IN ('retry_count', 'max_retries', 'last_retry_at');
-- Migration: Add neighborhood filter to get_posts_with_scores RPC
-- Run this in Supabase SQL Editor after 009_job_retry.sql
--
-- Adds p_neighborhood_id parameter to filter posts by neighborhood.

-- ============================================================================
-- RPC Function: get_posts_with_scores (with neighborhood filter)
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL
)
RETURNS TABLE(
    post_id UUID,
    final_score FLOAT,
    llm_score_id UUID,
    scores JSONB,
    categories TEXT[],
    summary TEXT,
    model_version TEXT,
    llm_created_at TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ps.post_id,
        ps.final_score,
        ls.id AS llm_score_id,
        ls.scores,
        ls.categories,
        ls.summary,
        ls.model_version,
        ls.created_at AS llm_created_at
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
    ORDER BY ps.final_score DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- RPC Function: get_posts_with_scores_count (with neighborhood filter)
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add reaction_count to posts table
-- Run this in Supabase SQL Editor after 010_neighborhood_filter.sql
--
-- Stores engagement metric from Nextdoor (reactions/likes) for potential ranking use.

ALTER TABLE posts ADD COLUMN IF NOT EXISTS reaction_count INT DEFAULT 0;
-- Migration: Add saved/starred flag to posts
-- Run this in Supabase SQL Editor after 011_reaction_count.sql
--
-- Allows users to save posts for later review (separate from used_on_episode).

ALTER TABLE posts ADD COLUMN IF NOT EXISTS saved BOOLEAN DEFAULT false;

CREATE INDEX IF NOT EXISTS idx_posts_saved ON posts(saved) WHERE saved = true;
-- Migration: Add saved filter to get_posts_with_scores RPC
-- Run this in Supabase SQL Editor after 012_saved_posts.sql
--
-- Adds p_saved_only parameter to filter posts by saved status.

-- ============================================================================
-- RPC Function: get_posts_with_scores (with saved filter)
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false
)
RETURNS TABLE(
    post_id UUID,
    final_score FLOAT,
    llm_score_id UUID,
    scores JSONB,
    categories TEXT[],
    summary TEXT,
    model_version TEXT,
    llm_created_at TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ps.post_id,
        ps.final_score,
        ls.id AS llm_score_id,
        ls.scores,
        ls.categories,
        ls.summary,
        ls.model_version,
        ls.created_at AS llm_created_at
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
    ORDER BY ps.final_score DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- RPC Function: get_posts_with_scores_count (with saved filter)
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add episode_date filter to get_posts_with_scores RPC
-- Run this in Supabase SQL Editor after 013_rpc_saved_filter.sql
--
-- Adds p_episode_date parameter to filter posts used in a specific episode.

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL
)
RETURNS TABLE(
    post_id UUID,
    final_score FLOAT,
    llm_score_id UUID,
    scores JSONB,
    categories TEXT[],
    summary TEXT,
    model_version TEXT,
    llm_created_at TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ps.post_id,
        ps.final_score,
        ls.id AS llm_score_id,
        ls.scores,
        ls.categories,
        ls.summary,
        ls.model_version,
        ls.created_at AS llm_created_at
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
    ORDER BY ps.final_score DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR p.episode_date = p_episode_date);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add full-text search support for keyword search
-- Run this in Supabase SQL Editor after 014_episode_date_filter.sql
--
-- Adds ts_vector column and GIN index for full-text search on post text.

ALTER TABLE posts ADD COLUMN IF NOT EXISTS text_search tsvector
  GENERATED ALWAYS AS (to_tsvector('english', coalesce(text, ''))) STORED;

CREATE INDEX IF NOT EXISTS idx_posts_text_search ON posts USING GIN(text_search);
-- Migration: Add why_podcast_worthy to llm_scores and RPC
-- Run this in Supabase SQL Editor after 015_fulltext_search.sql
--
-- Stores a one-sentence LLM explanation of why a post is good for the podcast.
-- podcast_worthy (1-10) is stored inside the existing scores JSONB.

ALTER TABLE llm_scores ADD COLUMN IF NOT EXISTS why_podcast_worthy TEXT;

-- Must drop first: PostgreSQL does not allow changing return type with CREATE OR REPLACE.
DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, date);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ls.categories,
        ls.created_at AS llm_created_at,
        ls.id AS llm_score_id,
        ls.model_version,
        ps.post_id,
        ps.final_score,
        ls.scores,
        ls.summary,
        ls.why_podcast_worthy
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
    ORDER BY ps.final_score DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add get_posts_by_date RPC for date-sorted feed with filters in DB
-- Run this in Supabase SQL Editor after 016_why_podcast_worthy.sql
--
-- Enables category and min_score filtering in the database when sort=date,
-- so pagination and count are correct.

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ls.categories,
        ls.created_at AS llm_created_at,
        ls.id AS llm_score_id,
        ls.model_version,
        p.id AS post_id,
        ls.final_score,
        ls.scores,
        ls.summary,
        ls.why_podcast_worthy
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR (p.episode_date = p_episode_date AND p.used_on_episode = true))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
    ORDER BY p.created_at DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION get_posts_by_date_count(
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR (p.episode_date = p_episode_date AND p.used_on_episode = true))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add get_embedding_backlog_count RPC for stats
-- Run this in Supabase SQL Editor after 017_posts_by_date_rpc.sql
--
-- Returns count of posts that have LLM scores but no embedding (search backlog).

CREATE OR REPLACE FUNCTION get_embedding_backlog_count()
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM llm_scores ls
    LEFT JOIN post_embeddings pe ON ls.post_id = pe.post_id
    WHERE pe.post_id IS NULL;

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add min_podcast_worthy filter and sort by podcast score
-- Run this in Supabase SQL Editor after 018_embedding_backlog_count.sql
--
-- Adds p_min_podcast_worthy and p_order_by to get_posts_with_scores for filtering
-- and sorting by the podcast_worthy dimension (stored in ls.scores JSONB).
-- Also adds p_min_podcast_worthy to get_posts_by_date.

-- ============================================================================
-- get_posts_with_scores: add p_min_podcast_worthy, p_order_by
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, date);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score'
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_by = 'podcast_worthy' THEN
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        ORDER BY (ls.scores->>'podcast_worthy')::float DESC NULLS LAST, ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    ELSE
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        ORDER BY ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- get_posts_with_scores_count: add p_min_podcast_worthy
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_min_podcast_worthy FLOAT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- get_posts_by_date: add p_min_podcast_worthy
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ls.categories,
        ls.created_at AS llm_created_at,
        ls.id AS llm_score_id,
        ls.model_version,
        p.id AS post_id,
        ls.final_score,
        ls.scores,
        ls.summary,
        ls.why_podcast_worthy
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR (p.episode_date = p_episode_date AND p.used_on_episode = true))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
    ORDER BY p.created_at DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- get_posts_by_date_count: add p_min_podcast_worthy
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_by_date_count(
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR (p.episode_date = p_episode_date AND p.used_on_episode = true))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add min_reaction_count filter to post feed RPCs
-- Run this in Supabase SQL Editor after 019_podcast_worthy_filter.sql
--
-- Adds p_min_reaction_count to get_posts_with_scores, get_posts_with_scores_count,
-- get_posts_by_date, and get_posts_by_date_count. Filters by posts.reaction_count
-- (from migration 011) so "High engagement" chip can show only posts with at least
-- N reactions.

-- ============================================================================
-- get_posts_with_scores: add p_min_reaction_count
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, date, double precision, text);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_by = 'podcast_worthy' THEN
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY (ls.scores->>'podcast_worthy')::float DESC NULLS LAST, ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    ELSE
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- get_posts_with_scores_count: add p_min_reaction_count
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR p.episode_date = p_episode_date)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- get_posts_by_date: add p_min_reaction_count
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ls.categories,
        ls.created_at AS llm_created_at,
        ls.id AS llm_score_id,
        ls.model_version,
        p.id AS post_id,
        ls.final_score,
        ls.scores,
        ls.summary,
        ls.why_podcast_worthy
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR (p.episode_date = p_episode_date AND p.used_on_episode = true))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
    ORDER BY p.created_at DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- get_posts_by_date_count: add p_min_reaction_count
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_by_date_count(
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_episode_date DATE DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_episode_date IS NULL OR (p.episode_date = p_episode_date AND p.used_on_episode = true))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;
-- Add comments JSONB to posts for scraper-extracted comment data
-- Run this in Supabase SQL Editor

ALTER TABLE posts
ADD COLUMN IF NOT EXISTS comments JSONB DEFAULT '[]';

COMMENT ON COLUMN posts.comments IS 'List of {author_name, text, timestamp_relative} from feed comment drawer';
-- Migration: Remove episode_date from posts (keep used_on_episode and mark-as-used)
-- Run this in Supabase SQL Editor after 021_posts_comments.sql
--
-- Drops only the episode_date column. All RPCs are recreated without p_episode_date
-- and without filtering/returning episode_date. used_on_episode is unchanged.

-- ============================================================================
-- Step 1: Drop episode_date column
-- ============================================================================

ALTER TABLE posts DROP COLUMN IF EXISTS episode_date;

-- ============================================================================
-- Step 2: get_posts_with_scores (remove p_episode_date param and WHERE clause)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, date, double precision, text, integer);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_by = 'podcast_worthy' THEN
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY (ls.scores->>'podcast_worthy')::float DESC NULLS LAST, ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    ELSE
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 3: get_posts_with_scores_count
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 4: get_posts_by_date (remove p_episode_date param and WHERE clause)
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ls.categories,
        ls.created_at AS llm_created_at,
        ls.id AS llm_score_id,
        ls.model_version,
        p.id AS post_id,
        ls.final_score,
        ls.scores,
        ls.summary,
        ls.why_podcast_worthy
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
    ORDER BY p.created_at DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 5: get_posts_by_date_count
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_by_date_count(
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 6: search_posts_by_embedding (remove episode_date from return)
-- ============================================================================

DROP FUNCTION IF EXISTS search_posts_by_embedding(vector, double precision, integer);

CREATE OR REPLACE FUNCTION search_posts_by_embedding(
    query_embedding VECTOR(1536),
    similarity_threshold FLOAT DEFAULT 0.5,
    result_limit INT DEFAULT 10
)
RETURNS TABLE(
    id UUID,
    text TEXT,
    similarity DOUBLE PRECISION,
    created_at TIMESTAMPTZ,
    neighborhood_id UUID,
    post_id_ext TEXT,
    url TEXT,
    user_id_hash TEXT,
    image_urls JSONB,
    hash TEXT,
    used_on_episode BOOLEAN
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        p.id,
        p.text,
        (1 - (pe.embedding <=> query_embedding))::DOUBLE PRECISION AS similarity,
        p.created_at,
        p.neighborhood_id,
        p.post_id_ext::TEXT,
        p.url::TEXT,
        p.user_id_hash::TEXT,
        p.image_urls,
        p.hash::TEXT,
        COALESCE(p.used_on_episode, false)
    FROM posts p
    INNER JOIN post_embeddings pe ON p.id = pe.post_id
    WHERE 1 - (pe.embedding <=> query_embedding) >= similarity_threshold
    ORDER BY pe.embedding <=> query_embedding
    LIMIT result_limit;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add ignored (soft delete) to posts
-- Run this in Supabase SQL Editor after 022_remove_episode_date_only.sql
--
-- Adds posts.ignored and p_ignored_only to feed RPCs. Default feed excludes
-- ignored posts; ignored_only=true shows only ignored. Search excludes ignored.

-- ============================================================================
-- Step 1: Add column and index
-- ============================================================================

ALTER TABLE posts ADD COLUMN IF NOT EXISTS ignored BOOLEAN DEFAULT false;

CREATE INDEX IF NOT EXISTS idx_posts_ignored ON posts(ignored) WHERE ignored = true;

-- ============================================================================
-- Step 2: get_posts_with_scores (add p_ignored_only)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, double precision, text, integer);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_by = 'podcast_worthy' THEN
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY (ls.scores->>'podcast_worthy')::float DESC NULLS LAST, ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    ELSE
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            ps.post_id,
            ps.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM post_scores ps
        INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
        INNER JOIN posts p ON ps.post_id = p.id
        WHERE ps.weight_config_id = p_weight_config_id
            AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY ps.final_score DESC
        LIMIT p_limit
        OFFSET p_offset;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 3: get_posts_with_scores_count (add p_ignored_only)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores_count(uuid, double precision, text, boolean, uuid, boolean, double precision, integer);

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_ignored_only = COALESCE(p.ignored, false))
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 4: get_posts_by_date (add p_ignored_only)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_by_date(integer, integer, text, double precision, uuid, boolean, boolean, double precision, integer);

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        ls.categories,
        ls.created_at AS llm_created_at,
        ls.id AS llm_score_id,
        ls.model_version,
        p.id AS post_id,
        ls.final_score,
        ls.scores,
        ls.summary,
        ls.why_podcast_worthy
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_ignored_only = COALESCE(p.ignored, false))
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
    ORDER BY p.created_at DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 5: get_posts_by_date_count (add p_ignored_only)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_by_date_count(text, double precision, uuid, boolean, boolean, double precision, integer);

CREATE OR REPLACE FUNCTION get_posts_by_date_count(
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_ignored_only = COALESCE(p.ignored, false))
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 6: search_posts_by_embedding (exclude ignored, no new param)
-- ============================================================================

DROP FUNCTION IF EXISTS search_posts_by_embedding(vector, double precision, integer);

CREATE OR REPLACE FUNCTION search_posts_by_embedding(
    query_embedding VECTOR(1536),
    similarity_threshold FLOAT DEFAULT 0.5,
    result_limit INT DEFAULT 10
)
RETURNS TABLE(
    id UUID,
    text TEXT,
    similarity DOUBLE PRECISION,
    created_at TIMESTAMPTZ,
    neighborhood_id UUID,
    post_id_ext TEXT,
    url TEXT,
    user_id_hash TEXT,
    image_urls JSONB,
    hash TEXT,
    used_on_episode BOOLEAN
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        p.id,
        p.text,
        (1 - (pe.embedding <=> query_embedding))::DOUBLE PRECISION AS similarity,
        p.created_at,
        p.neighborhood_id,
        p.post_id_ext::TEXT,
        p.url::TEXT,
        p.user_id_hash::TEXT,
        p.image_urls,
        p.hash::TEXT,
        COALESCE(p.used_on_episode, false)
    FROM posts p
    INNER JOIN post_embeddings pe ON p.id = pe.post_id
    WHERE 1 - (pe.embedding <=> query_embedding) >= similarity_threshold
        AND COALESCE(p.ignored, false) = false
    ORDER BY pe.embedding <=> query_embedding
    LIMIT result_limit;
END;
$$ LANGUAGE plpgsql;
-- Migration: Default session neighborhood and sessions unique constraint
-- Run this in Supabase SQL Editor after 023_posts_ignored.sql
--
-- Sessions table uses neighborhood_id UUID REFERENCES neighborhoods(id). The scraper
-- stores one "default" session when no specific neighborhood is selected. This
-- migration:
-- 1. Inserts a reserved "Default" neighborhood with a fixed UUID so the scraper
--    can store the default session (see SessionManager.DEFAULT_SESSION_ID in code).
-- 2. Adds a unique index on sessions(neighborhood_id) so upsert on_conflict
--    works and we keep at most one session per neighborhood.

-- ============================================================================
-- Step 1: Default neighborhood for scraper session storage
-- ============================================================================

INSERT INTO neighborhoods (id, name, slug, is_active, weight_modifier)
VALUES (
    '00000000-0000-0000-0000-000000000001',
    'Default',
    'default',
    true,
    1.0
)
ON CONFLICT (id) DO NOTHING;

-- ============================================================================
-- Step 2: Unique index on sessions(neighborhood_id) for upsert
-- ============================================================================

CREATE UNIQUE INDEX IF NOT EXISTS idx_sessions_neighborhood_id
    ON sessions (neighborhood_id);
-- Migration: Enable Row Level Security (RLS) on all tables
-- Run this in Supabase SQL Editor after 024_default_session_neighborhood.sql
--
-- Enables RLS on all public tables and creates policies for:
-- 1. Service role (service key) - full access (bypasses RLS but explicit is better)
-- 2. Authenticated users - read/write based on table needs
-- 3. Public - read-only for neighborhoods (if needed)
--
-- Note: Service key operations bypass RLS, but enabling RLS protects against
-- accidental client-side direct access and provides defense-in-depth.

-- ============================================================================
-- Step 1: Enable RLS on all tables
-- ============================================================================

ALTER TABLE neighborhoods ENABLE ROW LEVEL SECURITY;
ALTER TABLE sessions ENABLE ROW LEVEL SECURITY;
ALTER TABLE posts ENABLE ROW LEVEL SECURITY;
ALTER TABLE llm_scores ENABLE ROW LEVEL SECURITY;
ALTER TABLE post_embeddings ENABLE ROW LEVEL SECURITY;
ALTER TABLE post_scores ENABLE ROW LEVEL SECURITY;
ALTER TABLE weight_configs ENABLE ROW LEVEL SECURITY;
ALTER TABLE background_jobs ENABLE ROW LEVEL SECURITY;
ALTER TABLE topic_frequencies ENABLE ROW LEVEL SECURITY;
ALTER TABLE settings ENABLE ROW LEVEL SECURITY;

-- ============================================================================
-- Step 2: Policies for neighborhoods (read-only for anon role)
-- ============================================================================

-- Allow anon role to read neighborhoods (defensive - protects against accidental client access)
-- Note: All API routes use service key which bypasses RLS, so this only matters
-- if client-side code accidentally uses the anon key directly
CREATE POLICY "Anon can read neighborhoods"
    ON neighborhoods
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 3: Policies for posts (read-only for anon role)
-- ============================================================================

-- Allow anon role to read posts (defensive)
CREATE POLICY "Anon can read posts"
    ON posts
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 4: Policies for llm_scores (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read llm_scores"
    ON llm_scores
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 5: Policies for post_embeddings (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read post_embeddings"
    ON post_embeddings
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 6: Policies for post_scores (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read post_scores"
    ON post_scores
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 7: Policies for weight_configs (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read weight_configs"
    ON weight_configs
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 8: Policies for background_jobs (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read background_jobs"
    ON background_jobs
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 9: Policies for topic_frequencies (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read topic_frequencies"
    ON topic_frequencies
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 10: Policies for settings (read-only for anon role)
-- ============================================================================

CREATE POLICY "Anon can read settings"
    ON settings
    FOR SELECT
    TO anon
    USING (true);

-- ============================================================================
-- Step 11: Policies for sessions (service role only - scraper writes)
-- ============================================================================

-- Sessions are written by the scraper (service key) and should not be
-- accessible via client-side queries. No policies needed - service key
-- bypasses RLS, and we don't want client access.

-- ============================================================================
-- Note on service role access and write operations:
-- ============================================================================
-- The service role (SUPABASE_SERVICE_KEY) bypasses RLS automatically.
-- All write operations (INSERT, UPDATE, DELETE) from the scraper and
-- server-side API routes use the service key, so they will continue to work.
--
-- RLS policies above:
-- - Enable RLS on all tables (required for security)
-- - Allow read access via anon role (defensive - protects against accidental
--   client-side direct access if someone uses getSupabase() instead of API routes)
-- - Block all writes via anon role (only service key can write)
--
-- This provides defense-in-depth: even if client-side code accidentally uses
-- the anon key directly, it can only read data, not modify it.
-- Migration: Set search_path on all functions (fix "Function Search Path Mutable" warnings)
-- Run this in Supabase SQL Editor after 025_enable_rls.sql
--
-- Supabase security scanner flags functions without an explicit search_path.
-- Setting search_path = public prevents search_path injection and ensures
-- consistent behavior. All functions reference only public schema objects.

-- ============================================================================
-- Functions with no arguments
-- ============================================================================

ALTER FUNCTION update_updated_at_column() SET search_path = public;
ALTER FUNCTION recount_topic_frequencies() SET search_path = public;
ALTER FUNCTION update_weight_config_has_scores() SET search_path = public;
ALTER FUNCTION get_embedding_backlog_count() SET search_path = public;

-- ============================================================================
-- Functions with arguments (signatures must match exactly)
-- ============================================================================

ALTER FUNCTION increment_topic_frequency(text, int) SET search_path = public;
ALTER FUNCTION get_unscored_posts(int) SET search_path = public;

-- get_posts_with_scores(uuid, int, int, double precision, text, boolean, uuid, boolean, double precision, text, int, boolean)
ALTER FUNCTION get_posts_with_scores(uuid, int, int, double precision, text, boolean, uuid, boolean, double precision, text, int, boolean) SET search_path = public;

-- get_posts_with_scores_count(uuid, double precision, text, boolean, uuid, boolean, double precision, int, boolean)
ALTER FUNCTION get_posts_with_scores_count(uuid, double precision, text, boolean, uuid, boolean, double precision, int, boolean) SET search_path = public;

-- get_posts_by_date(int, int, text, double precision, uuid, boolean, boolean, double precision, int, boolean)
ALTER FUNCTION get_posts_by_date(int, int, text, double precision, uuid, boolean, boolean, double precision, int, boolean) SET search_path = public;

-- get_posts_by_date_count(text, double precision, uuid, boolean, boolean, double precision, int, boolean)
ALTER FUNCTION get_posts_by_date_count(text, double precision, uuid, boolean, boolean, double precision, int, boolean) SET search_path = public;

-- search_posts_by_embedding(vector, double precision, int)
ALTER FUNCTION search_posts_by_embedding(vector, double precision, int) SET search_path = public;
-- Add poster display name to posts (from scraper author_name).
-- Existing rows will have NULL; new scrapes will populate it.

ALTER TABLE posts
  ADD COLUMN IF NOT EXISTS author_name TEXT;

COMMENT ON COLUMN posts.author_name IS 'Display name of the post author from Nextdoor (scraper author_name).';
-- Migration: Add sort order (asc/desc) to feed RPCs
-- Run after 027_posts_author_name.sql
--
-- Adds p_order_asc to get_posts_with_scores and get_posts_by_date so the API
-- can request ascending order (e.g. lowest score first, oldest first).

-- ============================================================================
-- Step 1: get_posts_with_scores (add p_order_asc)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, double precision, text, integer, boolean);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_order_asc BOOLEAN DEFAULT false
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_by = 'podcast_worthy' THEN
        IF p_order_asc THEN
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            ORDER BY (ls.scores->>'podcast_worthy')::float ASC NULLS LAST, ps.final_score ASC
            LIMIT p_limit
            OFFSET p_offset;
        ELSE
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            ORDER BY (ls.scores->>'podcast_worthy')::float DESC NULLS LAST, ps.final_score DESC
            LIMIT p_limit
            OFFSET p_offset;
        END IF;
    ELSE
        IF p_order_asc THEN
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            ORDER BY ps.final_score ASC
            LIMIT p_limit
            OFFSET p_offset;
        ELSE
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            ORDER BY ps.final_score DESC
            LIMIT p_limit
            OFFSET p_offset;
        END IF;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 2: get_posts_by_date (add p_order_asc)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_by_date(integer, integer, text, double precision, uuid, boolean, boolean, double precision, integer);

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_order_asc BOOLEAN DEFAULT false
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_asc THEN
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            p.id AS post_id,
            ls.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM posts p
        INNER JOIN llm_scores ls ON p.id = ls.post_id
        WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY p.created_at ASC
        LIMIT p_limit
        OFFSET p_offset;
    ELSE
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            p.id AS post_id,
            ls.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM posts p
        INNER JOIN llm_scores ls ON p.id = ls.post_id
        WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        ORDER BY p.created_at DESC
        LIMIT p_limit
        OFFSET p_offset;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 3: Set search_path on new function signatures
-- ============================================================================

ALTER FUNCTION get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, double precision, text, integer, boolean, boolean) SET search_path = public;
ALTER FUNCTION get_posts_by_date(integer, integer, text, double precision, uuid, boolean, boolean, double precision, integer, boolean) SET search_path = public;
-- Add prompt_version to llm_scores for feedback loop and A/B tests

ALTER TABLE llm_scores
ADD COLUMN IF NOT EXISTS prompt_version TEXT;

COMMENT ON COLUMN llm_scores.prompt_version IS 'Version or hash of the scoring prompt that produced these scores';
-- Batch RPC to increment topic frequencies in one round-trip

CREATE OR REPLACE FUNCTION increment_topic_frequencies_batch(p_updates JSONB)
RETURNS VOID AS $$
DECLARE
    rec RECORD;
BEGIN
    FOR rec IN SELECT * FROM jsonb_to_recordset(p_updates)
        AS x(category TEXT, increment INT)
    LOOP
        UPDATE topic_frequencies
        SET count_30d = count_30d + rec.increment,
            last_updated = NOW()
        WHERE topic_frequencies.category = rec.category;

        IF NOT FOUND THEN
            INSERT INTO topic_frequencies (category, count_30d, last_updated)
            VALUES (rec.category, rec.increment, NOW())
            ON CONFLICT (category) DO UPDATE
            SET count_30d = topic_frequencies.count_30d + rec.increment,
                last_updated = NOW();
        END IF;
    END LOOP;
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION increment_topic_frequencies_batch(JSONB) IS
    'Batch increment topic frequencies; p_updates: [{"category": "drama", "increment": 3}, ...]';
-- RPC to return score distribution stats for tuning

CREATE OR REPLACE FUNCTION get_score_distribution()
RETURNS JSONB AS $$
DECLARE
    dim TEXT;
    dims TEXT[] := ARRAY[
        'absurdity', 'discussion_spark', 'drama', 'emotional_intensity',
        'news_value', 'podcast_worthy', 'readability'
    ];
    dim_stats JSONB;
    dims_obj JSONB := '{}'::jsonb;
    final_stats JSONB;
BEGIN
    -- Per-dimension stats from scores JSONB
    FOREACH dim IN ARRAY dims
    LOOP
        dim_stats := NULL;
        SELECT jsonb_build_object(
            'max', ROUND(MAX((scores->>dim)::float)::numeric, 2),
            'mean', ROUND(AVG((scores->>dim)::float)::numeric, 2),
            'min', ROUND(MIN((scores->>dim)::float)::numeric, 2),
            'p50', ROUND((percentile_cont(0.5) WITHIN GROUP (ORDER BY (scores->>dim)::float))::numeric, 2),
            'p90', ROUND((percentile_cont(0.9) WITHIN GROUP (ORDER BY (scores->>dim)::float))::numeric, 2)
        ) INTO dim_stats
        FROM llm_scores
        WHERE scores->>dim IS NOT NULL
          AND (scores->>dim) ~ '^[0-9]+\.?[0-9]*$';

        IF dim_stats IS NOT NULL THEN
            dims_obj := dims_obj || jsonb_build_object(dim, dim_stats);
        END IF;
    END LOOP;

    -- final_score stats
    SELECT jsonb_build_object(
        'max', ROUND(MAX(final_score)::numeric, 2),
        'mean', ROUND(AVG(final_score)::numeric, 2),
        'min', ROUND(MIN(final_score)::numeric, 2),
        'p50', ROUND((percentile_cont(0.5) WITHIN GROUP (ORDER BY final_score))::numeric, 2),
        'p90', ROUND((percentile_cont(0.9) WITHIN GROUP (ORDER BY final_score))::numeric, 2)
    ) INTO final_stats
    FROM llm_scores
    WHERE final_score IS NOT NULL;

    RETURN jsonb_build_object(
        'dimensions', dims_obj,
        'final_score', COALESCE(final_stats, '{}'::jsonb)
    );
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION get_score_distribution() IS
    'Returns min, max, mean, p50, p90 per dimension and final_score for tuning';
-- Migration: Post Scores Staging for Clean Cutover
-- Run after 031_get_score_distribution.sql
--
-- Enables recompute jobs to write to a staging table and apply all scores
-- in one transaction when complete. Avoids mixing old and new scores in
-- the feed during a long recompute.

-- ============================================================================
-- Step 1: Create post_scores_staging table
-- ============================================================================

CREATE TABLE post_scores_staging (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES background_jobs(id) ON DELETE CASCADE,
    post_id UUID NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
    weight_config_id UUID NOT NULL REFERENCES weight_configs(id) ON DELETE CASCADE,
    final_score FLOAT NOT NULL,
    computed_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(job_id, post_id)
);

CREATE INDEX idx_post_scores_staging_job ON post_scores_staging(job_id);
CREATE INDEX idx_post_scores_staging_config_post ON post_scores_staging(weight_config_id, post_id);

-- ============================================================================
-- Step 2: Create cutover RPC
-- ============================================================================

-- Apply post_scores from staging in one transaction.
-- Call only on job success. Deletes existing post_scores for the config,
-- inserts from staging, then clears staging for this job.
CREATE OR REPLACE FUNCTION apply_post_scores_from_staging(
    p_job_id UUID,
    p_weight_config_id UUID
)
RETURNS void
LANGUAGE plpgsql
AS $$
BEGIN
    -- All in one transaction: delete old, insert from staging, clear staging
    DELETE FROM post_scores
    WHERE weight_config_id = p_weight_config_id;

    INSERT INTO post_scores (post_id, weight_config_id, final_score, computed_at)
    SELECT post_id, weight_config_id, final_score, computed_at
    FROM post_scores_staging
    WHERE job_id = p_job_id;

    DELETE FROM post_scores_staging
    WHERE job_id = p_job_id;
END;
$$;
-- Migration: Runtime Score Calculation RPC
-- Run after 032_post_scores_staging.sql
--
-- Enables preview mode: compute final_score from llm_scores + weight config
-- (weights + novelty) without reading post_scores. Used when preview=true.

-- ============================================================================
-- Step 1: Helper function compute_final_score_runtime
-- ============================================================================

-- Port of Python novelty.py calculate_novelty + worker calculate_final_score.
-- Returns final_score (0-10) for a post given llm_scores and config.
CREATE OR REPLACE FUNCTION compute_final_score_runtime(
    p_scores JSONB,
    p_categories TEXT[],
    p_weights JSONB,
    p_novelty_config JSONB,
    p_frequencies JSONB,
    p_total_scored INT
)
RETURNS FLOAT
LANGUAGE plpgsql
IMMUTABLE
AS $$
DECLARE
    v_novelty FLOAT := 1.0;
    v_min_mult FLOAT;
    v_max_mult FLOAT;
    v_rare INT;
    v_common INT;
    v_very_common INT;
    v_total_freq FLOAT := 0;
    v_avg_freq FLOAT;
    v_weighted_sum FLOAT := 0;
    v_max_possible FLOAT := 0;
    v_normalized FLOAT;
    v_raw_score FLOAT;
    v_dim TEXT;
    v_cat TEXT;
    v_count INT;
    v_thresholds JSONB;
BEGIN
    -- Cold start: categories empty, frequencies empty, or total scored < 30
    IF p_categories IS NULL OR array_length(p_categories, 1) IS NULL THEN
        v_novelty := 1.0;
    ELSIF p_frequencies IS NULL OR p_frequencies = '{}'::jsonb THEN
        v_novelty := 1.0;
    ELSIF p_total_scored IS NOT NULL AND p_total_scored < 30 THEN
        v_novelty := 1.0;
    ELSE
        -- Compute novelty from categories and frequencies
        v_min_mult := COALESCE((p_novelty_config->>'min_multiplier')::float, 0.2);
        v_max_mult := COALESCE((p_novelty_config->>'max_multiplier')::float, 1.5);
        v_thresholds := COALESCE(p_novelty_config->'frequency_thresholds', '{}'::jsonb);
        v_rare := COALESCE((v_thresholds->>'rare')::int, 5);
        v_common := COALESCE((v_thresholds->>'common')::int, 30);
        v_very_common := COALESCE((v_thresholds->>'very_common')::int, 100);

        FOR v_cat IN SELECT unnest(p_categories)
        LOOP
            v_count := COALESCE((p_frequencies->>v_cat)::int, 0);
            v_total_freq := v_total_freq + v_count;
        END LOOP;

        v_avg_freq := v_total_freq / NULLIF(array_length(p_categories, 1), 0);

        IF v_avg_freq <= v_rare THEN
            v_novelty := v_max_mult;
        ELSIF v_avg_freq <= v_common THEN
            v_novelty := v_max_mult - ((v_avg_freq - v_rare)::float / NULLIF(v_common - v_rare, 0)) * (v_max_mult - 1.0);
        ELSIF v_avg_freq <= v_very_common THEN
            v_novelty := 1.0 - ((v_avg_freq - v_common)::float / NULLIF(v_very_common - v_common, 0)) * (1.0 - v_min_mult);
        ELSE
            v_novelty := v_min_mult;
        END IF;
    END IF;

    -- Compute weighted sum and max_possible from weights
    FOR v_dim IN SELECT jsonb_object_keys(p_weights)
    LOOP
        v_weighted_sum := v_weighted_sum + COALESCE((p_scores->>v_dim)::float, 5.0) * (p_weights->>v_dim)::float;
        v_max_possible := v_max_possible + 10.0 * (p_weights->>v_dim)::float;
    END LOOP;

    IF v_max_possible <= 0 THEN
        RETURN 0.0;
    END IF;

    v_normalized := (v_weighted_sum / v_max_possible) * 10.0;
    v_raw_score := v_normalized * v_novelty;

    RETURN GREATEST(0.0, LEAST(10.0, v_raw_score));
END;
$$;

-- ============================================================================
-- Step 2: get_posts_with_runtime_scores RPC
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_runtime_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_order_asc BOOLEAN DEFAULT false
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_weights JSONB;
    v_novelty_config JSONB;
    v_frequencies JSONB;
    v_total_scored INT;
BEGIN
    -- Load config once
    SELECT wc.weights INTO v_weights
    FROM weight_configs wc
    WHERE wc.id = p_weight_config_id;

    IF v_weights IS NULL THEN
        RETURN;
    END IF;

    SELECT COALESCE(s.value::jsonb, '{}'::jsonb) INTO v_novelty_config
    FROM settings s
    WHERE s.key = 'novelty_config'
    LIMIT 1;

    SELECT COALESCE(jsonb_object_agg(tf.category, tf.count_30d), '{}'::jsonb) INTO v_frequencies
    FROM topic_frequencies tf;

    SELECT COUNT(*)::int INTO v_total_scored
    FROM llm_scores;

    IF p_order_by = 'podcast_worthy' THEN
        IF p_order_asc THEN
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
            ORDER BY (s.ls_scores->>'podcast_worthy')::float ASC NULLS LAST, s.fs ASC
            LIMIT p_limit OFFSET p_offset;
        ELSE
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
            ORDER BY (s.ls_scores->>'podcast_worthy')::float DESC NULLS LAST, s.fs DESC
            LIMIT p_limit OFFSET p_offset;
        END IF;
    ELSE
        IF p_order_asc THEN
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
            ORDER BY s.fs ASC
            LIMIT p_limit OFFSET p_offset;
        ELSE
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
            ORDER BY s.fs DESC
            LIMIT p_limit OFFSET p_offset;
        END IF;
    END IF;
END;
$$;

-- ============================================================================
-- Step 3: get_posts_with_runtime_scores_count RPC
-- ============================================================================

CREATE OR REPLACE FUNCTION get_posts_with_runtime_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false
)
RETURNS INT
LANGUAGE plpgsql
AS $$
DECLARE
    v_weights JSONB;
    v_novelty_config JSONB;
    v_frequencies JSONB;
    v_total_scored INT;
    v_count INT;
BEGIN
    SELECT wc.weights INTO v_weights
    FROM weight_configs wc
    WHERE wc.id = p_weight_config_id;

    IF v_weights IS NULL THEN
        RETURN 0;
    END IF;

    SELECT COALESCE(s.value::jsonb, '{}'::jsonb) INTO v_novelty_config
    FROM settings s
    WHERE s.key = 'novelty_config'
    LIMIT 1;

    SELECT COALESCE(jsonb_object_agg(tf.category, tf.count_30d), '{}'::jsonb) INTO v_frequencies
    FROM topic_frequencies tf;

    SELECT COUNT(*)::int INTO v_total_scored
    FROM llm_scores;

    SELECT COUNT(*)::int INTO v_count
    FROM (
        SELECT compute_final_score_runtime(
            ls.scores, ls.categories, v_weights,
            v_novelty_config, v_frequencies, v_total_scored
        ) AS fs
        FROM posts p
        INNER JOIN llm_scores ls ON p.id = ls.post_id
        WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
    ) sub
    WHERE (p_min_score IS NULL OR sub.fs >= p_min_score);

    RETURN v_count;
END;
$$;
-- Migration: Add get_posts_without_embeddings RPC for chunked embedder
-- Run this in Supabase SQL Editor after 033_runtime_scores_rpc.sql
--
-- Returns posts that have text but no embedding (for chunked processing).
-- Limit parameter controls chunk size; ORDER BY id for stable pagination.

CREATE OR REPLACE FUNCTION get_posts_without_embeddings(lim INT DEFAULT 500)
RETURNS TABLE (id UUID, text TEXT) AS $$
  SELECT p.id, p.text
  FROM posts p
  LEFT JOIN post_embeddings pe ON p.id = pe.post_id
  WHERE pe.post_id IS NULL
    AND p.text IS NOT NULL
    AND trim(p.text) != ''
  ORDER BY p.id
  LIMIT lim;
$$ LANGUAGE sql STABLE;
-- Migration: Scraper runs table for self-reported scrape outcomes
-- Run this in Supabase SQL Editor after 034_get_posts_without_embeddings.sql
--
-- The scraper inserts one row per run (success or failure) so the Jobs page
-- can show last N days of scrape runs without Healthchecks.io.

-- ============================================================================
-- Step 1: Create scraper_runs table
-- ============================================================================

CREATE TABLE scraper_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    run_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    status TEXT NOT NULL CHECK (status IN ('completed', 'error')),
    feed_type TEXT NOT NULL CHECK (feed_type IN ('recent', 'trending')),
    error_message TEXT
);

CREATE INDEX idx_scraper_runs_run_at ON scraper_runs(run_at DESC);

COMMENT ON TABLE scraper_runs IS 'Self-reported scrape run outcomes from the scraper pipeline (one row per run).';

-- ============================================================================
-- RLS: no policies for anon/authenticated (service role bypasses RLS)
-- ============================================================================

ALTER TABLE scraper_runs ENABLE ROW LEVEL SECURITY;
-- Migration: Backfill dimension RPCs for scoring
-- Run this in Supabase SQL Editor after 035_scraper_runs.sql
--
-- When a new scoring dimension is added to SCORING_DIMENSIONS (scraper),
-- existing llm_scores rows lack that key. Create a backfill_dimension job via
-- POST /api/admin/backfill-dimension with body { dimension: "new_key" }; the
-- worker will score only that dimension and merge into scores (other keys unchanged).

-- ============================================================================
-- Step 1: get_posts_missing_dimension
-- ============================================================================
-- Returns (id, text) for posts that have llm_scores but scores->p_dimension
-- is null or missing. Used by the backfill_dimension worker to fetch batches.

CREATE OR REPLACE FUNCTION get_posts_missing_dimension(
    p_dimension TEXT,
    p_limit INT DEFAULT 100
)
RETURNS TABLE(id UUID, text TEXT) AS $$
BEGIN
    RETURN QUERY
    SELECT p.id, p.text
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (ls.scores->>p_dimension) IS NULL
       OR NOT (ls.scores ? p_dimension)
    ORDER BY ls.created_at ASC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 2: merge_dimension_into_llm_scores
-- ============================================================================
-- p_updates: JSONB array of {"post_id": "uuid", "value": number}.
-- Merges the single dimension into each row's scores (other keys unchanged).

CREATE OR REPLACE FUNCTION merge_dimension_into_llm_scores(
    p_dimension TEXT,
    p_updates JSONB
)
RETURNS VOID AS $$
BEGIN
    UPDATE llm_scores ls
    SET scores = ls.scores || jsonb_build_object(
        p_dimension,
        (elem->>'value')::float
    )
    FROM jsonb_array_elements(p_updates) AS elem
    WHERE ls.post_id = (elem->>'post_id')::uuid;
END;
$$ LANGUAGE plpgsql;
-- Migration: Add max range filters to feed RPCs
-- Run after 028_feed_sort_order.sql
--
-- Adds p_max_score, p_max_podcast_worthy, p_max_reaction_count to
-- get_posts_with_scores, get_posts_with_scores_count, get_posts_by_date,
-- get_posts_by_date_count so the filter menu min/max ranges work.

-- ============================================================================
-- Step 1: get_posts_with_scores (add p_max_*)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, double precision, text, integer, boolean, boolean);

CREATE OR REPLACE FUNCTION get_posts_with_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_order_asc BOOLEAN DEFAULT false,
    p_max_score FLOAT DEFAULT NULL,
    p_max_podcast_worthy FLOAT DEFAULT NULL,
    p_max_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_by = 'podcast_worthy' THEN
        IF p_order_asc THEN
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_max_score IS NULL OR ps.final_score <= p_max_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            ORDER BY (ls.scores->>'podcast_worthy')::float ASC NULLS LAST, ps.final_score ASC
            LIMIT p_limit
            OFFSET p_offset;
        ELSE
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_max_score IS NULL OR ps.final_score <= p_max_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            ORDER BY (ls.scores->>'podcast_worthy')::float DESC NULLS LAST, ps.final_score DESC
            LIMIT p_limit
            OFFSET p_offset;
        END IF;
    ELSE
        IF p_order_asc THEN
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_max_score IS NULL OR ps.final_score <= p_max_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            ORDER BY ps.final_score ASC
            LIMIT p_limit
            OFFSET p_offset;
        ELSE
            RETURN QUERY
            SELECT
                ls.categories,
                ls.created_at AS llm_created_at,
                ls.id AS llm_score_id,
                ls.model_version,
                ps.post_id,
                ps.final_score,
                ls.scores,
                ls.summary,
                ls.why_podcast_worthy
            FROM post_scores ps
            INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
            INNER JOIN posts p ON ps.post_id = p.id
            WHERE ps.weight_config_id = p_weight_config_id
                AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
                AND (p_max_score IS NULL OR ps.final_score <= p_max_score)
                AND (p_category IS NULL OR p_category = ANY(ls.categories))
                AND (NOT p_unused_only OR p.used_on_episode = false)
                AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                AND (p_ignored_only = COALESCE(p.ignored, false))
                AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            ORDER BY ps.final_score DESC
            LIMIT p_limit
            OFFSET p_offset;
        END IF;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 2: get_posts_with_scores_count (add p_max_*)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_scores_count(uuid, double precision, text, boolean, uuid, boolean, double precision, integer, boolean);

CREATE OR REPLACE FUNCTION get_posts_with_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_max_score FLOAT DEFAULT NULL,
    p_max_podcast_worthy FLOAT DEFAULT NULL,
    p_max_reaction_count INT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM post_scores ps
    INNER JOIN llm_scores ls ON ps.post_id = ls.post_id
    INNER JOIN posts p ON ps.post_id = p.id
    WHERE ps.weight_config_id = p_weight_config_id
        AND (p_min_score IS NULL OR ps.final_score >= p_min_score)
        AND (p_max_score IS NULL OR ps.final_score <= p_max_score)
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (p_ignored_only = COALESCE(p.ignored, false))
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 3: get_posts_by_date (add p_max_*)
-- ============================================================================

-- Drop the 11-param version from 028 (includes p_order_asc); 028 DROP targeted wrong 9-param version.
DROP FUNCTION IF EXISTS get_posts_by_date(integer, integer, text, double precision, uuid, boolean, boolean, double precision, integer, boolean, boolean);

CREATE OR REPLACE FUNCTION get_posts_by_date(
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_order_asc BOOLEAN DEFAULT false,
    p_max_score FLOAT DEFAULT NULL,
    p_max_podcast_worthy FLOAT DEFAULT NULL,
    p_max_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
) AS $$
BEGIN
    IF p_order_asc THEN
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            p.id AS post_id,
            ls.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM posts p
        INNER JOIN llm_scores ls ON p.id = ls.post_id
        WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
            AND (p_max_score IS NULL OR ls.final_score <= p_max_score)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
        ORDER BY p.created_at ASC
        LIMIT p_limit
        OFFSET p_offset;
    ELSE
        RETURN QUERY
        SELECT
            ls.categories,
            ls.created_at AS llm_created_at,
            ls.id AS llm_score_id,
            ls.model_version,
            p.id AS post_id,
            ls.final_score,
            ls.scores,
            ls.summary,
            ls.why_podcast_worthy
        FROM posts p
        INNER JOIN llm_scores ls ON p.id = ls.post_id
        WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
            AND (p_max_score IS NULL OR ls.final_score <= p_max_score)
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
        ORDER BY p.created_at DESC
        LIMIT p_limit
        OFFSET p_offset;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 4: get_posts_by_date_count (add p_max_*)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_by_date_count(text, double precision, uuid, boolean, boolean, double precision, integer, boolean);

CREATE OR REPLACE FUNCTION get_posts_by_date_count(
    p_category TEXT DEFAULT NULL,
    p_min_score FLOAT DEFAULT NULL,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_unused_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_max_score FLOAT DEFAULT NULL,
    p_max_podcast_worthy FLOAT DEFAULT NULL,
    p_max_reaction_count INT DEFAULT NULL
)
RETURNS INT AS $$
DECLARE
    result_count INT;
BEGIN
    SELECT COUNT(*) INTO result_count
    FROM posts p
    INNER JOIN llm_scores ls ON p.id = ls.post_id
    WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
        AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
        AND (NOT p_unused_only OR p.used_on_episode = false)
        AND (p_ignored_only = COALESCE(p.ignored, false))
        AND (p_category IS NULL OR p_category = ANY(ls.categories))
        AND (p_min_score IS NULL OR ls.final_score >= p_min_score)
        AND (p_max_score IS NULL OR ls.final_score <= p_max_score)
        AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
        AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
        AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
        AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count);

    RETURN result_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Step 5: Set search_path on new function signatures
-- ============================================================================

ALTER FUNCTION get_posts_with_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, double precision, text, integer, boolean, boolean, double precision, double precision, integer) SET search_path = public;
ALTER FUNCTION get_posts_with_scores_count(uuid, double precision, text, boolean, uuid, boolean, double precision, integer, boolean, double precision, double precision, integer) SET search_path = public;
ALTER FUNCTION get_posts_by_date(integer, integer, text, double precision, uuid, boolean, boolean, double precision, integer, boolean, double precision, double precision, integer) SET search_path = public;
ALTER FUNCTION get_posts_by_date_count(text, double precision, uuid, boolean, boolean, double precision, integer, boolean, double precision, double precision, integer) SET search_path = public;

-- ============================================================================
-- Step 6: get_posts_with_runtime_scores (add p_max_*) for preview mode
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_runtime_scores(uuid, integer, integer, double precision, text, boolean, uuid, boolean, double precision, text, integer, boolean, boolean);

CREATE OR REPLACE FUNCTION get_posts_with_runtime_scores(
    p_weight_config_id UUID,
    p_limit INT DEFAULT 20,
    p_offset INT DEFAULT 0,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_order_by TEXT DEFAULT 'score',
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_order_asc BOOLEAN DEFAULT false,
    p_max_score FLOAT DEFAULT NULL,
    p_max_podcast_worthy FLOAT DEFAULT NULL,
    p_max_reaction_count INT DEFAULT NULL
)
RETURNS TABLE(
    categories TEXT[],
    llm_created_at TIMESTAMPTZ,
    llm_score_id UUID,
    model_version TEXT,
    post_id UUID,
    final_score FLOAT,
    scores JSONB,
    summary TEXT,
    why_podcast_worthy TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_weights JSONB;
    v_novelty_config JSONB;
    v_frequencies JSONB;
    v_total_scored INT;
BEGIN
    SELECT wc.weights INTO v_weights
    FROM weight_configs wc
    WHERE wc.id = p_weight_config_id;

    IF v_weights IS NULL THEN
        RETURN;
    END IF;

    SELECT COALESCE(s.value::jsonb, '{}'::jsonb) INTO v_novelty_config
    FROM settings s
    WHERE s.key = 'novelty_config'
    LIMIT 1;

    SELECT COALESCE(jsonb_object_agg(tf.category, tf.count_30d), '{}'::jsonb) INTO v_frequencies
    FROM topic_frequencies tf;

    SELECT COUNT(*)::int INTO v_total_scored
    FROM llm_scores;

    IF p_order_by = 'podcast_worthy' THEN
        IF p_order_asc THEN
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                    AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
                AND (p_max_score IS NULL OR s.fs <= p_max_score)
            ORDER BY (s.ls_scores->>'podcast_worthy')::float ASC NULLS LAST, s.fs ASC
            LIMIT p_limit OFFSET p_offset;
        ELSE
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                    AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
                AND (p_max_score IS NULL OR s.fs <= p_max_score)
            ORDER BY (s.ls_scores->>'podcast_worthy')::float DESC NULLS LAST, s.fs DESC
            LIMIT p_limit OFFSET p_offset;
        END IF;
    ELSE
        IF p_order_asc THEN
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                    AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
                AND (p_max_score IS NULL OR s.fs <= p_max_score)
            ORDER BY s.fs ASC
            LIMIT p_limit OFFSET p_offset;
        ELSE
            RETURN QUERY
            WITH scored AS (
                SELECT
                    ls.categories,
                    ls.created_at AS ls_created_at,
                    ls.id AS ls_id,
                    ls.model_version,
                    p.id AS p_id,
                    ls.scores AS ls_scores,
                    ls.summary AS ls_summary,
                    ls.why_podcast_worthy AS ls_why,
                    compute_final_score_runtime(
                        ls.scores, ls.categories, v_weights,
                        v_novelty_config, v_frequencies, v_total_scored
                    ) AS fs
                FROM posts p
                INNER JOIN llm_scores ls ON p.id = ls.post_id
                WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
                    AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
                    AND (NOT p_unused_only OR p.used_on_episode = false)
                    AND (p_ignored_only = COALESCE(p.ignored, false))
                    AND (p_category IS NULL OR p_category = ANY(ls.categories))
                    AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
                    AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
                    AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
                    AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
            )
            SELECT
                s.categories,
                s.ls_created_at AS llm_created_at,
                s.ls_id AS llm_score_id,
                s.model_version,
                s.p_id AS post_id,
                s.fs AS final_score,
                s.ls_scores AS scores,
                s.ls_summary AS summary,
                s.ls_why AS why_podcast_worthy
            FROM scored s
            WHERE (p_min_score IS NULL OR s.fs >= p_min_score)
                AND (p_max_score IS NULL OR s.fs <= p_max_score)
            ORDER BY s.fs DESC
            LIMIT p_limit OFFSET p_offset;
        END IF;
    END IF;
END;
$$;

-- ============================================================================
-- Step 7: get_posts_with_runtime_scores_count (add p_max_*)
-- ============================================================================

DROP FUNCTION IF EXISTS get_posts_with_runtime_scores_count(uuid, double precision, text, boolean, uuid, boolean, double precision, integer, boolean);

CREATE OR REPLACE FUNCTION get_posts_with_runtime_scores_count(
    p_weight_config_id UUID,
    p_min_score FLOAT DEFAULT NULL,
    p_category TEXT DEFAULT NULL,
    p_unused_only BOOLEAN DEFAULT false,
    p_neighborhood_id UUID DEFAULT NULL,
    p_saved_only BOOLEAN DEFAULT false,
    p_min_podcast_worthy FLOAT DEFAULT NULL,
    p_min_reaction_count INT DEFAULT NULL,
    p_ignored_only BOOLEAN DEFAULT false,
    p_max_score FLOAT DEFAULT NULL,
    p_max_podcast_worthy FLOAT DEFAULT NULL,
    p_max_reaction_count INT DEFAULT NULL
)
RETURNS INT
LANGUAGE plpgsql
AS $$
DECLARE
    v_weights JSONB;
    v_novelty_config JSONB;
    v_frequencies JSONB;
    v_total_scored INT;
    v_count INT;
BEGIN
    SELECT wc.weights INTO v_weights
    FROM weight_configs wc
    WHERE wc.id = p_weight_config_id;

    IF v_weights IS NULL THEN
        RETURN 0;
    END IF;

    SELECT COALESCE(s.value::jsonb, '{}'::jsonb) INTO v_novelty_config
    FROM settings s
    WHERE s.key = 'novelty_config'
    LIMIT 1;

    SELECT COALESCE(jsonb_object_agg(tf.category, tf.count_30d), '{}'::jsonb) INTO v_frequencies
    FROM topic_frequencies tf;

    SELECT COUNT(*)::int INTO v_total_scored
    FROM llm_scores;

    SELECT COUNT(*)::int INTO v_count
    FROM (
        SELECT compute_final_score_runtime(
            ls.scores, ls.categories, v_weights,
            v_novelty_config, v_frequencies, v_total_scored
        ) AS fs
        FROM posts p
        INNER JOIN llm_scores ls ON p.id = ls.post_id
        WHERE (p_neighborhood_id IS NULL OR p.neighborhood_id = p_neighborhood_id)
            AND (NOT p_saved_only OR COALESCE(p.saved, false) = true)
            AND (NOT p_unused_only OR p.used_on_episode = false)
            AND (p_ignored_only = COALESCE(p.ignored, false))
            AND (p_category IS NULL OR p_category = ANY(ls.categories))
            AND (p_min_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float >= p_min_podcast_worthy)
            AND (p_max_podcast_worthy IS NULL OR (ls.scores->>'podcast_worthy')::float <= p_max_podcast_worthy)
            AND (p_min_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) >= p_min_reaction_count)
            AND (p_max_reaction_count IS NULL OR COALESCE(p.reaction_count, 0) <= p_max_reaction_count)
    ) sub
    WHERE (p_min_score IS NULL OR sub.fs >= p_min_score)
        AND (p_max_score IS NULL OR sub.fs <= p_max_score);

    RETURN v_count;
END;
$$;
